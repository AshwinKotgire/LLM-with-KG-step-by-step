{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ0hDUOnAVlvIHuU9dObT3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshwinKotgire/LLM-with-KG-step-by-step/blob/Prop_rel_for_all_nodes/Chatting_with_KG__with__UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q\n",
        "!pip install langchain -q\n",
        "!pip install retry -q\n",
        "!pip install neo4j\n",
        "!pip install -qU langchain-openai\n",
        "!pip install pyngrok\n",
        "!pip install streamlit\n",
        "!pip install streamlit\n",
        "!pip install -qU langchain-openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f920FG9Nd-W3",
        "outputId": "3467fe18-d7e6-4ab8-876c-bb141be8d56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.19.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.4)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.33.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.11.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.8.1b0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.33.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.11.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.8.1b0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF  # For reading PDF content\n",
        "!pip install PyPDF2   # Alternative library for PDF manipulation\n",
        "!pip install openai -q\n",
        "!pip install langchain\n",
        "\n",
        "!pip install retry\n",
        "!pip install openai -q\n",
        "!pip install langchain -q\n",
        "!pip install retry -q\n",
        "!pip install neo4j\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjfFA-A8OZQ1",
        "outputId": "87a5ca27-301b-4879-c70d-7fdbc6340cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.1 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.24.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.32)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.42)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.47)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: retry in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry) (4.4.2)\n",
            "Requirement already satisfied: py<2.0.0,>=1.4.26 in /usr/local/lib/python3.10/dist-packages (from retry) (1.11.0)\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.19.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import json\n",
        "import os\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain.llms import Ollama\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOllama\n",
        "from langchain import PromptTemplate\n",
        "# from langchain.openai import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List,Dict\n",
        "\n",
        "# Imports from other local python files\n",
        "# from NEO4J_Graph import Graph\n",
        "# from FHIR_to_graph import resource_to_node, resource_to_edges\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "os.environ[\"NEO4J_URI\"] = 'neo4j+s://4469cbdc.databases.neo4j.io'\n",
        "os.environ[\"NEO4J_USERNAME\"] = 'neo4j'\n",
        "os.environ[\"NEO4J_PASSWORD\"] = '2mFdM_33Bx5KUR_2v2UcX7itdIJVNyZC9blMM5vHWvA'\n",
        "\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import json\n",
        "from retry import retry\n",
        "# import os\n",
        "client = OpenAI()\n",
        "\n",
        "og= Neo4jGraph(url=os.environ[\"NEO4J_URI\"], username=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"])\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") #embeddings.embed_query(text)\n"
      ],
      "metadata": {
        "id": "ozK-S2VUgrPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class agent:\n",
        "  def __init__(self,client=None,model=\"gpt-3.5-turbo-0125\",prompt_template=None):\n",
        "    self.client=client\n",
        "    self.model=model\n",
        "    self.messages=[]\n",
        "    self.prompt_template=prompt_template\n",
        "  def prompt_from_template(self,template,dict1):\n",
        "    try :\n",
        "      return template.format(**dict1)\n",
        "    except Exception as e:\n",
        "      return \"Not valid dictionary for the tempalte\"\n",
        "\n",
        "  def run_prompt(self,prompt)    :\n",
        "    response = self.client.chat.completions.create(\n",
        "         model=self.model,\n",
        "         messages=[{\"role\": \"system\", \"content\": \"You are Mutual Funds Support Copilot for OpenXfi.\"},\n",
        "          {\"role\":\"user\",\"content\":prompt}])\n",
        "    self.messages=[]\n",
        "    self.messages=[{\"role\":\"user\",\"content\":prompt},\n",
        "     {\"role\":\"assistant\",\"content\":response.choices[0].message.content}]\n",
        "    print(len(self.messages))\n",
        "    return response.choices[0].message.content\n",
        "  def run_are_u_sure(self,extention=None)  :\n",
        "    self.messages.append({\"role\":\"user\",\"content\":\"Please Re-evaluate.\"+extention})\n",
        "    response = self.client.chat.completions.create(\n",
        "         model=self.model,\n",
        "         messages=self.messages)\n",
        "    self.messages=[{\"role\":\"user\",\"content\":\"Please Re-evaluate.\"+extention},{\"role\":\"user\",\"content\":response.choices[0].message.content}]\n",
        "    return response.choices[0].message.content\n",
        "  def extract_from_string(self,string_)  :\n",
        "    # end=string_.find('<')\n",
        "    matches=string_\n",
        "    if('json' in string_) :\n",
        "      pattern = r'```json\\s*([\\s\\S]+?)\\s*```'\n",
        "    # Find all matches\n",
        "      matches = re.findall(pattern, string_)[0]\n",
        "    elif('python' in string_):\n",
        "      pattern = r'```python\\s*([\\s\\S]+?)\\s*```'\n",
        "      matches = re.findall(pattern, string_)[0]\n",
        "\n",
        "    list_string = matches[:]\n",
        "    output_list = eval(list_string)\n",
        "    return output_list\n",
        "  @retry(exceptions=Exception, tries=3, delay=1)\n",
        "  def complete_run(self,template,question:Dict,are_you_sure=None,verbose=False)  :\n",
        "    prompt=self.prompt_from_template(template,question)\n",
        "    o=self.run_prompt(prompt)\n",
        "    if(verbose==True):\n",
        "      print(o)\n",
        "    # time.sleep(20)\n",
        "    if(are_you_sure!=None):\n",
        "      o=self.run_are_u_sure(are_you_sure)\n",
        "      # time.sleep(20)\n",
        "      print(o)\n",
        "    o=self.extract_from_string(o)\n",
        "    # print(o)\n",
        "    return o\n",
        "\n"
      ],
      "metadata": {
        "id": "T76GQtIxFRPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_kg(agent_to_chat:agent,kg,vector_name_index,question:str,verbose=False):\n",
        "  propmpt_ex_node_names=\"\"\" You are a Mutual Funds Support Copilot for Openxfi who uses Knowldege graph for refence.\n",
        "  You will be propived with a question by the customer and you have to extract all the entities from the question which can be used to query the knowledge graph and extract context.\n",
        "  Return output in jollowing JSON format:: json```{{'entities':['entity 1','entity 2', ...]}}```\n",
        "  Question:{question} \"\"\"\n",
        "  Question=question\n",
        "  list_of_entities=agent_to_chat.complete_run(propmpt_ex_node_names,{'question':Question})\n",
        "\n",
        "  def name_vector_index_search(kg,node_names,index_name,k=4,verbose=False):\n",
        "    ce=[]\n",
        "    cypher=''\n",
        "\n",
        "    for node in node_names:\n",
        "      embedding= embeddings.embed_query(node)\n",
        "      o=kg.query(f\"call db.index.vector.queryNodes('{index_name}',{k},{embedding}) yield node,score return node as node\")\n",
        "      # print (o)\n",
        "      for i in o:\n",
        "        ce.append((i['node']['name'],i['node']['type']))\n",
        "      # ce.append(o)\n",
        "    return list(set(ce))\n",
        "  out_vec=name_vector_index_search(kg,list_of_entities['entities'],vector_name_index)\n",
        "  if(verbose==True):print('Nearest nodes::',out_vec)\n",
        "\n",
        "  def extraData(kg,node_names):\n",
        "    output=[]\n",
        "    cypher =\"\"\"MATCH (base_node)-[relationship]-(connected_node)\n",
        "      WHERE base_node.name = '{node_name}' and base_node.type='{type}'\n",
        "      RETURN base_node.name, type(relationship) AS relationship_name, connected_node.name, connected_node.type\n",
        "        \"\"\"\n",
        "    base_node_cypher=\"\"\"MATCH (base_node) WHERE base_node.name = '{node_name}' and base_node.type='{type}' RETURN base_node.name,base_node.type,base_node.text \"\"\"\n",
        "    i=0\n",
        "    for name,type_ in node_names:\n",
        "      base_d=kg.query(base_node_cypher.format(**{'node_name':name,'type':type_}))\n",
        "      output.append(base_d)\n",
        "      o=kg.query(cypher.format(**{'node_name':name,'type':type_}))\n",
        "      output.append(o)\n",
        "    return output\n",
        "\n",
        "  ex_data=extraData(kg,out_vec)\n",
        "  if(verbose==True):print('refernce data::',ex_data)\n",
        "\n",
        "  propmpt_ex_node_names=\"\"\" You are a Mutual Funds Support Copilot for Openxfi who uses Knowldege graph for refence.\n",
        "  You were provided with a question by the customer and you had to extract all the entities from the question which could be used to query the knowledge graph and extract context.\n",
        "  After that those were used to query graph and get context.\n",
        "  Following is the context extracted. All the relationships the node has with other nodes is extracted and given in context in JSON format. Use it to answer the question.\n",
        "  Answer the question based on context in Natual Language only.\n",
        "\n",
        "  Question:{question}\n",
        "  \\nContext::\\n{context}\n",
        "  Important: Answer truthfully and to the point and just say I dont know if you do not.Do not say 'Based on context provided' EVER. Instead talk like a human would .\n",
        "  \"\"\"\n",
        "\n",
        "  output=agent_to_chat.run_prompt(agent_to_chat.prompt_from_template(propmpt_ex_node_names,{'question':Question,'context':ex_data}))\n",
        "  return output\n",
        "agent_to_chat=agent(client)\n",
        "answer=chat_with_kg(agent_to_chat,og,'index_name_emb','What are Arbitrage Mutual Funds')\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "sehZHSUfFk8d",
        "outputId": "91a09f1b-b8b4-4136-93b0-59be784bcc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Arbitrage mutual funds involve buying and selling securities in different markets to profit from price differences. These funds are suitable for low-risk investors looking to benefit from market volatility. They are subject to taxation but may have certain tax exemptions based on specific terms and conditions. Arbitrage mutual funds offer an investment route that exploits arbitrage opportunities and mitigates risk, making them an alternative to traditional investment options like bank FDs. They are a type of investment activity that generates returns by focusing on the value in both future and cash markets. Arbitrage mutual funds are supported by various financial institutions and can help investors achieve their financial goals. They are exemplified by various funds such as SBI Arbitrage Opportunities Fund, Baroda BNP Paribas Arbitrage Fund, and others, making them a popular choice among investors.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chatbot.py\n",
        "import streamlit as st\n",
        "import random\n",
        "import time\n",
        "\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain.llms import Ollama\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOllama\n",
        "from langchain import PromptTemplate\n",
        "# from langchain.openai import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List,Dict\n",
        "\n",
        "# Imports from other local python files\n",
        "# from NEO4J_Graph import Graph\n",
        "# from FHIR_to_graph import resource_to_node, resource_to_edges\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "os.environ[\"NEO4J_URI\"] = 'neo4j+s://4469cbdc.databases.neo4j.io'\n",
        "os.environ[\"NEO4J_USERNAME\"] = 'neo4j'\n",
        "os.environ[\"NEO4J_PASSWORD\"] = '2mFdM_33Bx5KUR_2v2UcX7itdIJVNyZC9blMM5vHWvA'\n",
        "\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import json\n",
        "from retry import retry\n",
        "# import os\n",
        "client = OpenAI()\n",
        "# @st.cache_resource\n",
        "# # def db_eb():\n",
        "og= Neo4jGraph(url=os.environ[\"NEO4J_URI\"], username=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"])\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") #embeddings.embed_query(text)\n",
        "  # return og,embeddings\n",
        "# og,embeddings=db_eb()\n",
        "\n",
        "class agent:\n",
        "  def __init__(self,client=None,model=\"gpt-3.5-turbo-0125\",prompt_template=None):\n",
        "    self.client=client\n",
        "    self.model=model\n",
        "    self.messages=[]\n",
        "    self.prompt_template=prompt_template\n",
        "  def prompt_from_template(self,template,dict1):\n",
        "    try :\n",
        "      return template.format(**dict1)\n",
        "    except Exception as e:\n",
        "      return \"Not valid dictionary for the tempalte\"\n",
        "\n",
        "  def run_prompt(self,prompt)    :\n",
        "    response = self.client.chat.completions.create(\n",
        "         model=self.model,\n",
        "         messages=[{\"role\": \"system\", \"content\": \"You are Mutual Funds Support Copilot for OpenXfi.\"},\n",
        "          {\"role\":\"user\",\"content\":prompt}])\n",
        "    self.messages=[]\n",
        "    self.messages=[{\"role\":\"user\",\"content\":prompt},\n",
        "     {\"role\":\"assistant\",\"content\":response.choices[0].message.content}]\n",
        "    print(len(self.messages))\n",
        "    return response.choices[0].message.content\n",
        "  def run_are_u_sure(self,extention=None)  :\n",
        "    self.messages.append({\"role\":\"user\",\"content\":\"Please Re-evaluate.\"+extention})\n",
        "    response = self.client.chat.completions.create(\n",
        "         model=self.model,\n",
        "         messages=self.messages)\n",
        "    self.messages=[{\"role\":\"user\",\"content\":\"Please Re-evaluate.\"+extention},{\"role\":\"user\",\"content\":response.choices[0].message.content}]\n",
        "    return response.choices[0].message.content\n",
        "  def extract_from_string(self,string_)  :\n",
        "    # end=string_.find('<')\n",
        "    matches=string_\n",
        "    if('json' in string_) :\n",
        "      pattern = r'```json\\s*([\\s\\S]+?)\\s*```'\n",
        "    # Find all matches\n",
        "      matches = re.findall(pattern, string_)[0]\n",
        "    elif('python' in string_):\n",
        "      pattern = r'```python\\s*([\\s\\S]+?)\\s*```'\n",
        "      matches = re.findall(pattern, string_)[0]\n",
        "\n",
        "    list_string = matches[:]\n",
        "    output_list = eval(list_string)\n",
        "    return output_list\n",
        "  @retry(exceptions=Exception, tries=3, delay=1)\n",
        "  def complete_run(self,template,question:Dict,are_you_sure=None,verbose=False)  :\n",
        "    prompt=self.prompt_from_template(template,question)\n",
        "    o=self.run_prompt(prompt)\n",
        "    if(verbose==True):\n",
        "      print(o)\n",
        "    # time.sleep(20)\n",
        "    if(are_you_sure!=None):\n",
        "      o=self.run_are_u_sure(are_you_sure)\n",
        "      # time.sleep(20)\n",
        "      print(o)\n",
        "    o=self.extract_from_string(o)\n",
        "    # print(o)\n",
        "    return o\n",
        "\n",
        "def chat_with_kg(agent_to_chat:agent,kg,vector_name_index,question:str,verbose=False):\n",
        "  propmpt_ex_node_names=\"\"\" You are a Mutual Funds Support Copilot for Openxfi who uses Knowldege graph for refence.\n",
        "  You will be propived with a question by the customer and you have to extract all the entities from the question which can be used to query the knowledge graph and extract context.\n",
        "  Return output in jollowing JSON format:: json```{{'entities':['entity 1','entity 2', ...]}}```\n",
        "  Question:{question} \"\"\"\n",
        "  Question=question\n",
        "  list_of_entities=agent_to_chat.complete_run(propmpt_ex_node_names,{'question':Question})\n",
        "\n",
        "  def name_vector_index_search(kg,node_names,index_name,k=4,verbose=False):\n",
        "    ce=[]\n",
        "    cypher=''\n",
        "\n",
        "    for node in node_names:\n",
        "      embedding= embeddings.embed_query(node)\n",
        "      o=kg.query(f\"call db.index.vector.queryNodes('{index_name}',{k},{embedding}) yield node,score return node as node\")\n",
        "      # print (o)\n",
        "      for i in o:\n",
        "        ce.append((i['node']['name'],i['node']['type']))\n",
        "      # ce.append(o)\n",
        "    return list(set(ce))\n",
        "  out_vec=name_vector_index_search(kg,list_of_entities['entities'],vector_name_index)\n",
        "  if(verbose==True):print('Nearest nodes::',out_vec)\n",
        "\n",
        "  def extraData(kg,node_names):\n",
        "    output=[]\n",
        "    cypher =\"\"\"MATCH (base_node)-[relationship]-(connected_node)\n",
        "      WHERE base_node.name = '{node_name}' and base_node.type='{type}'\n",
        "      RETURN base_node.name, type(relationship) AS relationship_name, connected_node.name, connected_node.type\n",
        "        \"\"\"\n",
        "    base_node_cypher=\"\"\"MATCH (base_node) WHERE base_node.name = '{node_name}' and base_node.type='{type}' RETURN base_node.name,base_node.type,base_node.text \"\"\"\n",
        "    i=0\n",
        "    for name,type_ in node_names:\n",
        "      base_d=kg.query(base_node_cypher.format(**{'node_name':name,'type':type_}))\n",
        "      output.append(base_d)\n",
        "      o=kg.query(cypher.format(**{'node_name':name,'type':type_}))\n",
        "      output.append(o)\n",
        "    return output\n",
        "\n",
        "  ex_data=extraData(kg,out_vec)\n",
        "  if(verbose==True):print('refernce data::',ex_data)\n",
        "\n",
        "  propmpt_ex_node_names=\"\"\" You are a Mutual Funds Support Copilot for Openxfi who uses Knowldege graph for refence.\n",
        "  You were provided with a question by the customer and you had to extract all the entities from the question which could be used to query the knowledge graph and extract context.\n",
        "  After that those were used to query graph and get context.\n",
        "  Following is the context extracted. All the relationships the node has with other nodes is extracted and given in context in JSON format. Use it to answer the question.\n",
        "  Answer the question based on context in Natual Language only.\n",
        "\n",
        "  Question:{question}\n",
        "  \\nContext::\\n{context}\n",
        "  Important: Answer truthfully and to the point and just say I dont know if you do not.Do not say 'Based on context provided' EVER. Instead talk like a human would .\n",
        "  \"\"\"\n",
        "\n",
        "  output=agent_to_chat.run_prompt(agent_to_chat.prompt_from_template(propmpt_ex_node_names,{'question':Question,'context':ex_data}))\n",
        "  return output\n",
        "agent_to_chat=agent(client)\n",
        "# answer=chat_with_kg(agent_to_chat,og,'index_name_emb','What are Arbitrage Mutual Funds')\n",
        "\n",
        "\n",
        "# Streamed response emulator\n",
        "def response_generator(prompt):\n",
        "    answer=chat_with_kg(agent_to_chat,og,'index_name_emb',prompt)\n",
        "    return answer\n",
        "\n",
        "\n",
        "st.title('ðŸ¤– Chatfi as Support Copilot ðŸ¤–')\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Accept user input\n",
        "if prompt := st.chat_input(\"Please enter your question here?\"):\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        response = response_generator(prompt)\n",
        "        st.write(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGYZjlnzHR2f",
        "outputId": "759d0988-15b6-4b32-ace8-3d2470cd0224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing chatbot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chatbot.py\n",
        "import streamlit as st\n",
        "import random\n",
        "import time\n",
        "\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain.llms import Ollama\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOllama\n",
        "from langchain import PromptTemplate\n",
        "# from langchain.openai import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List,Dict\n",
        "from retry import retry\n",
        "from os import listdir\n",
        "\n",
        "# Imports from other local python files\n",
        "# from NEO4J_Graph import Graph\n",
        "# from FHIR_to_graph import resource_to_node, resource_to_edges\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "os.environ[\"NEO4J_URI\"] = 'neo4j+s://4469cbdc.databases.neo4j.io'\n",
        "os.environ[\"NEO4J_USERNAME\"] = 'neo4j'\n",
        "os.environ[\"NEO4J_PASSWORD\"] = '2mFdM_33Bx5KUR_2v2UcX7itdIJVNyZC9blMM5vHWvA'\n",
        "\n",
        "\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import json\n",
        "from retry import retry\n",
        "# import os\n",
        "client = OpenAI()\n",
        "# @st.cache_resource\n",
        "# # def db_eb():\n",
        "og= Neo4jGraph(url=os.environ[\"NEO4J_URI\"], username=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"])\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") #embeddings.embed_query(text)\n",
        "  # return og,embeddings\n",
        "# og,embeddings=db_eb()\n",
        "\n",
        "class agent:\n",
        "  def __init__(self,client=None,model=\"gpt-3.5-turbo-0125\",prompt_template=None):\n",
        "    self.client=client\n",
        "    self.model=model\n",
        "    self.messages=[]\n",
        "    self.prompt_template=prompt_template\n",
        "  def prompt_from_template(self,template,dict1):\n",
        "    try :\n",
        "      return template.format(**dict1)\n",
        "    except Exception as e:\n",
        "      return \"Not valid dictionary for the tempalte\"\n",
        "\n",
        "  def run_prompt(self,prompt)    :\n",
        "    response = self.client.chat.completions.create(\n",
        "         model=self.model,\n",
        "         messages=[{\"role\": \"system\", \"content\": \"You are Mutual Funds Support Copilot for OpenXfi.\"},\n",
        "          {\"role\":\"user\",\"content\":prompt}])\n",
        "    self.messages=[]\n",
        "    self.messages=[{\"role\":\"user\",\"content\":prompt},\n",
        "     {\"role\":\"assistant\",\"content\":response.choices[0].message.content}]\n",
        "    print(len(self.messages))\n",
        "    return response.choices[0].message.content\n",
        "  def run_are_u_sure(self,extention=None)  :\n",
        "    self.messages.append({\"role\":\"user\",\"content\":\"Please Re-evaluate.\"+extention})\n",
        "    response = self.client.chat.completions.create(\n",
        "         model=self.model,\n",
        "         messages=self.messages)\n",
        "    self.messages=[{\"role\":\"user\",\"content\":\"Please Re-evaluate.\"+extention},{\"role\":\"user\",\"content\":response.choices[0].message.content}]\n",
        "    return response.choices[0].message.content\n",
        "  def extract_from_string(self,string_)  :\n",
        "    # end=string_.find('<')\n",
        "    matches=string_\n",
        "    if('json' in string_) :\n",
        "      pattern = r'```json\\s*([\\s\\S]+?)\\s*```'\n",
        "    # Find all matches\n",
        "      matches = re.findall(pattern, string_)[0]\n",
        "    elif('python' in string_):\n",
        "      pattern = r'```python\\s*([\\s\\S]+?)\\s*```'\n",
        "      matches = re.findall(pattern, string_)[0]\n",
        "\n",
        "    list_string = matches[:]\n",
        "    output_list = eval(list_string)\n",
        "    return output_list\n",
        "  @retry(exceptions=Exception, tries=3, delay=1)\n",
        "  def complete_run(self,template,question:Dict,are_you_sure=None,verbose=False)  :\n",
        "    prompt=self.prompt_from_template(template,question)\n",
        "    o=self.run_prompt(prompt)\n",
        "    if(verbose==True):\n",
        "      print(o)\n",
        "    # time.sleep(20)\n",
        "    if(are_you_sure!=None):\n",
        "      o=self.run_are_u_sure(are_you_sure)\n",
        "      # time.sleep(20)\n",
        "      print(o)\n",
        "    o=self.extract_from_string(o)\n",
        "    # print(o)\n",
        "    return o\n",
        "agent_=agent(client)\n",
        "directory='/content/All Data'\n",
        "files=listdir(directory)\n",
        "\n",
        "def chat_with_kg(agent_to_chat:agent,kg,vector_name_index,question:str,verbose=False):\n",
        "  prompt=\"\"\"You are provided with the names of list of documents and a question. You have to tell which document can have the answer to the question.\n",
        "  Answer in this JSON format : {{'Most rel Doc':MOST RELEVANT DOCUMENT}}\n",
        "  Question :{question}\n",
        "  List of Documents :{doc_list}\n",
        "  \"\"\"\n",
        "  question=question\n",
        "\n",
        "  most_rd=agent_.complete_run(prompt,{'question':question,'doc_list':files}).get('Most rel Doc')\n",
        "  print(most_rd)\n",
        "  if(most_rd!=\"Answer on your own.\"):\n",
        "    file_path=directory+'/'+most_rd\n",
        "    data=open(file_path, 'r').read()\n",
        "    data=data.replace(\"Groww\", \"Openxfi\")\n",
        "  else:\n",
        "    data=\"\"\"I currently am unable to answer your query. Please contact Openxfi @ support@Openxfi.com , Sorry for your Inconvenience.\"\"\"\n",
        "  system_message=\"\"\"Groww is a financial technology company based in India that offers an online platform for individuals to invest in mutual funds, stocks, exchange-traded funds (ETFs), and other investment products. Founded in 2024, Groww aims to simplify the investment process and make it accessible to a wide range of people, including those who may not have extensive financial knowledge or experience.\n",
        "  The platform provides users with tools and resources to research and analyze various investment options, as well as the ability to invest directly through the platform. Groww's user-friendly interface, educational content, and low fees have contributed to its popularity among retail investors in India.\n",
        "  As of my last update, Groww has grown rapidly and has become one of the leading investment platforms in India, attracting millions of users. It continues to expand its offerings and services to cater to the evolving needs of investors.\n",
        "  \"\"\".replace(\"Groww\", \"Openxfi\")\n",
        "  p2=f\"\"\"{system_message}\n",
        "  You are a Mutual Funds Support Copilot for Openxfi who uses Knowldege graph for refence.\n",
        "    You were provided with a question by the customer and you had to extract all the entities from the question which could be used to query the knowledge graph and extract context.\n",
        "    After that those were used to query graph and get context.\n",
        "    Following is the context extracted.Use it to answer the question.\n",
        "    Answer the question based on context in Natual Language only.\n",
        "    Important: Answer truthfully and to the point and just say I dont know if you do not.\n",
        "    Important :: Do not use 'Based on context provided' EVER. Instead use 'As per my best knowledge'.\n",
        "    Question:{question}\n",
        "    \\nContext::\\n{data}\n",
        "  \"\"\"\n",
        "\n",
        "  answer=agent_.run_prompt(p2)\n",
        "  return answer\n",
        "agent_to_chat=agent(client)\n",
        "# answer=chat_with_kg(agent_to_chat,og,'index_name_emb','What are Arbitrage Mutual Funds')\n",
        "\n",
        "\n",
        "# Streamed response emulator\n",
        "def response_generator(prompt):\n",
        "    answer=chat_with_kg(agent_to_chat,og,'index_name_emb',prompt)\n",
        "    return answer\n",
        "\n",
        "\n",
        "st.title('ðŸ¤– ChatFi as Support Copilot ðŸ¤–')\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Accept user input\n",
        "if prompt := st.chat_input(\"Please enter your question here?\"):\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        response = response_generator(prompt)\n",
        "        st.write(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHsNcgDA4VSv",
        "outputId": "80563212-d224-4f2c-fdbb-38ff6ed06f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting chatbot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qfaaJHkCbcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF  # For reading PDF content\n",
        "!pip install PyPDF2   # Alternative library for PDF manipulation\n",
        "!pip install openai -q\n",
        "!pip install langchain\n",
        "\n",
        "!pip install retry\n",
        "!pip install openai -q\n",
        "!pip install langchain -q\n",
        "!pip install retry -q\n",
        "!pip install neo4j\n",
        "!pip install pyngrok\n",
        "!pip install streamlit\n",
        "!pip install -qU langchain-openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrv-TtrxCa99",
        "outputId": "4c349644-434a-4e9c-b8db-09029c234a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.1-cp310-none-manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.1 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.1 PyMuPDFb-1.24.1\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.33-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.43-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.1/289.1 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.48-py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.33 langchain-core-0.1.43 langchain-text-splitters-0.0.1 langsmith-0.1.48 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 typing-inspect-0.9.0\n",
            "Collecting retry\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry)\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py, retry\n",
            "Successfully installed py-1.11.0 retry-0.9.2\n",
            "Collecting neo4j\n",
            "  Downloading neo4j-5.19.0.tar.gz (202 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.4)\n",
            "Building wheels for collected packages: neo4j\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.19.0-py3-none-any.whl size=280741 sha256=09d437201015768a1b0f7bff4d4e13306fc658cbe60fbef550a2cda7bf5b2246\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/db/9b/2cfde1fa33145219c0322f299b604daf5aba2ed443a7ed5f07\n",
            "Successfully built neo4j\n",
            "Installing collected packages: neo4j\n",
            "Successfully installed neo4j-5.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUVLAyaTCdX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chatbot.py\n",
        "import streamlit as st\n",
        "import random\n",
        "import time\n",
        "\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain.llms import Ollama\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOllama\n",
        "from langchain import PromptTemplate\n",
        "# from langchain.openai import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List,Dict\n",
        "from retry import retry\n",
        "from os import listdir\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from typing import List,Dict\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "os.environ[\"NEO4J_URI\"] = 'neo4j+s://39944eda.databases.neo4j.io'\n",
        "os.environ[\"NEO4J_USERNAME\"] = 'neo4j'\n",
        "os.environ[\"NEO4J_PASSWORD\"] = 'cPhru78a_ORF-prPynqZ3XjUEXjaY5Z1V4ist9H1AV4'\n",
        "vector_index = Neo4jVector.from_existing_index(\n",
        "    embedding=embeddings,\n",
        "    url=os.environ[\"NEO4J_URI\"], username=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"],index_name='Index_embeddings')\n",
        "# Imports from other local python files\n",
        "# from NEO4J_Graph import Graph\n",
        "# from FHIR_to_graph import resource_to_node, resource_to_edges\n",
        "\n",
        "\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import json\n",
        "from retry import retry\n",
        "# import os\n",
        "client = OpenAI()\n",
        "# @st.cache_resource\n",
        "# # def db_eb():\n",
        "og= Neo4jGraph(url=os.environ[\"NEO4J_URI\"], username=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"])\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") #embeddings.embed_query(text)\n",
        "  # return og,embeddings\n",
        "# og,embeddings=db_eb()\n",
        "\n",
        "class agent:\n",
        "  def __init__(self,client=None,model=\"gpt-3.5-turbo-0125\",prompt_template=None):\n",
        "    self.client=client\n",
        "    self.model=model\n",
        "    self.messages=[]\n",
        "    self.prompt_template=prompt_template\n",
        "  def prompt_from_template(self,template,dict1):\n",
        "    try :\n",
        "      return template.format(**dict1)\n",
        "    except Exception as e:\n",
        "      return \"Not valid dictionary for the tempalte\"\n",
        "\n",
        "  def run_prompt(self,prompt)    :\n",
        "    response = self.client.chat.completions.create(\n",
        "         model=self.model,\n",
        "         messages=[{\"role\": \"system\", \"content\": \"You are Mutual Funds Support Copilot for OpenXfi.\"},\n",
        "          {\"role\":\"user\",\"content\":prompt}])\n",
        "    self.messages=[]\n",
        "    self.messages=[{\"role\":\"user\",\"content\":prompt},\n",
        "     {\"role\":\"assistant\",\"content\":response.choices[0].message.content}]\n",
        "    print(len(self.messages))\n",
        "    return response.choices[0].message.content\n",
        "  def run_are_u_sure(self,extention=None)  :\n",
        "    self.messages.append({\"role\":\"user\",\"content\":\"Please Re-evaluate.\"+extention})\n",
        "    response = self.client.chat.completions.create(\n",
        "         model=self.model,\n",
        "         messages=self.messages)\n",
        "    self.messages=[{\"role\":\"user\",\"content\":\"Please Re-evaluate.\"+extention},{\"role\":\"user\",\"content\":response.choices[0].message.content}]\n",
        "    return response.choices[0].message.content\n",
        "  def extract_from_string(self,string_)  :\n",
        "    # end=string_.find('<')\n",
        "    matches=string_\n",
        "    if('json' in string_) :\n",
        "      pattern = r'```json\\s*([\\s\\S]+?)\\s*```'\n",
        "    # Find all matches\n",
        "      matches = re.findall(pattern, string_)[0]\n",
        "    elif('python' in string_):\n",
        "      pattern = r'```python\\s*([\\s\\S]+?)\\s*```'\n",
        "      matches = re.findall(pattern, string_)[0]\n",
        "\n",
        "    list_string = matches[:]\n",
        "    output_list = eval(list_string)\n",
        "    return output_list\n",
        "  @retry(exceptions=Exception, tries=3, delay=1)\n",
        "  def complete_run(self,template,question:Dict,are_you_sure=None,verbose=False)  :\n",
        "    prompt=self.prompt_from_template(template,question)\n",
        "    o=self.run_prompt(prompt)\n",
        "    if(verbose==True):\n",
        "      print(o)\n",
        "    # time.sleep(20)\n",
        "    if(are_you_sure!=None):\n",
        "      o=self.run_are_u_sure(are_you_sure)\n",
        "      # time.sleep(20)\n",
        "      print(o)\n",
        "    o=self.extract_from_string(o)\n",
        "    # print(o)\n",
        "    return o\n",
        "agent_=agent(client)\n",
        "\n",
        "\n",
        "def chat_with_kg(agent_to_chat:agent,kg,vector_name_index,question:str,verbose=False):\n",
        "\n",
        "  prompt=question\n",
        "  docs_with_score = vector_index.similarity_search_with_score(prompt, k=10)\n",
        "\n",
        "  question=question\n",
        "\n",
        "  docs=[]\n",
        "  for d,s in docs_with_score:\n",
        "    docs.append(d.page_content.replace('Groww','Openxfi'))\n",
        "\n",
        "\n",
        "  p2=f\"\"\"\n",
        "  You are a Mutual Funds Support Copilot for Openxfi who uses Knowldege graph for refence.\n",
        "    You were provided with a question by the customer and you had to extract all the entities from the question which could be used to query the knowledge graph and extract context.\n",
        "    After that those were used to query graph and get context.\n",
        "    Following is the context extracted.Use it to answer the question.\n",
        "    Answer the question based on context in Natual Language only.\n",
        "    Important: Answer truthfully and to the point and just say I dont know if you do not.\n",
        "    Important :: DONOT use 'Based on context provided' EVER. Instead use 'As per my best knowledge'.\n",
        "    Very Important: If you think the answer is insuffucient to the question reply with \" If you think this doesnt answer your query you can contact hello@openxfi.com .\"\n",
        "\n",
        "    Question:{question}\n",
        "    \\Data::\\n{docs}\n",
        "  \"\"\"\n",
        "\n",
        "  answer=agent_to_chat.run_prompt(p2)\n",
        "  return answer\n",
        "agent_to_chat=agent(client)\n",
        "# answer=chat_with_kg(agent_to_chat,og,'index_name_emb','What are Arbitrage Mutual Funds')\n",
        "\n",
        "\n",
        "# Streamed response emulator\n",
        "def response_generator(prompt):\n",
        "    answer=chat_with_kg(agent_to_chat,og,'index_name_emb',prompt)\n",
        "    return answer\n",
        "\n",
        "\n",
        "st.title('ðŸ¤– ChatFi as Support Copilot ðŸ¤–')\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Accept user input\n",
        "if prompt := st.chat_input(\"Please enter your question here?\"):\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        response = response_generator(prompt)\n",
        "        st.write(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoiOe7AKBsRQ",
        "outputId": "b0047d5c-8314-4948-bdfa-52039ef83115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting chatbot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chatbot.py\n",
        "\n",
        "import streamlit as st\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "# Streamed response emulator\n",
        "def response_generator():\n",
        "    response = random.choice(\n",
        "        [\n",
        "            \"Hello there! How can I assist you today?\",\n",
        "            \"Hi, human! Is there anything I can help you with?\",\n",
        "            \"Do you need help?\",\n",
        "        ]\n",
        "    )\n",
        "    for word in response.split():\n",
        "        yield word + \" \"\n",
        "        time.sleep(0.05)\n",
        "\n",
        "\n",
        "st.title(\"Simple chat\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Accept user input\n",
        "if prompt := st.chat_input(\"What is up?\"):\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        response = st.write_stream(response_generator())\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNWURhLr64K",
        "outputId": "f64dec2e-6c02-45fa-e862-b947b885be02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting chatbot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tIw_zxtmL9AU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd24a4f3-4f00-47d3-9c8a-eb8f196485f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token('2dtWRnmxvDmAVmmNcc3xeX9pTc8_3TF4SRkSg5nhjoHMAKrTY')\n",
        "!nohup streamlit run chatbot.py --server.port 400 &\n",
        "\n",
        "url = ngrok.connect('400')\n",
        "print(url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrrlNMbqL-vw",
        "outputId": "c3a0ccf8-3609-4e12-cdc5-085deacc634e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "NgrokTunnel: \"https://8a1d-34-106-30-104.ngrok-free.app\" -> \"http://localhost:400\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "os.environ[\"NEO4J_URI\"] = 'neo4j+s://39944eda.databases.neo4j.io'\n",
        "os.environ[\"NEO4J_USERNAME\"] = 'neo4j'\n",
        "os.environ[\"NEO4J_PASSWORD\"] = 'cPhru78a_ORF-prPynqZ3XjUEXjaY5Z1V4ist9H1AV4'"
      ],
      "metadata": {
        "id": "yleujhwBVRta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}